---
# Name of the application
app_name: genai-chapterize-meeting-transcripts

aws:
  region: us-east-1

# directory paths
dir:
  data: data
  raw: data/source_data
  processed: data/processed_data
  completions: data/title_completions
  golden: data/source_data/golden
  prompts: data/prompts
  metrics: data/metrics
  processed_file: processed.csv
  chapterized_file: chapterized.csv
  metrics_file: per_request_results.csv
  summary_metrics_file: summary_metrics.csv
  model_evals_file: model_eval.csv
  file_type_to_process: vtt

# section that enables main.py to run each notebook without manually executing each of them
run_steps:
    0_chapterize_data.ipynb: yes
    1_generate_chapter_titles.ipynb: yes
    2_summarize_metrics.ipynb: yes

inference_parameters: 
  temperature: 0.1
  caching: False

title_generation_thresholds:
  max_chapter_length: 20 ## set a cap of the length of each chapter to be set for title generation

# bedrock FMs
experiments:
- name: chapterize-meeting-transcripts
  prompt_template:
  model_list:
  - model: mistral.mistral-7b-instruct-v0:2
    prompt_template: mistral_template.txt
    max_context_window: 32000
    input_tokens_pricing: 0.00015
    output_tokens_pricing: 0.0002
  - model: anthropic.claude-3-haiku-20240307-v1:0
    prompt_template: anthropic_template.txt
    max_context_window: 200000
    input_tokens_pricing: 0.00025
    output_tokens_pricing: 0.00125
  - model: anthropic.claude-3-sonnet-20240229-v1:0
    max_context_window: 200000
    prompt_template: anthropic_template.txt
    input_tokens_pricing: 0.00300
    output_tokens_pricing: 0.01500
  - model: amazon.titan-text-express-v1
    max_context_window: 8000
    prompt_template: titan_template.txt
    input_tokens_pricing: 0.0008
    output_tokens_pricing: 0.0016
  - model: meta.llama2-13b-chat-v1
    max_context_window: 4096
    prompt_template: llama_template.txt
    input_tokens_pricing: 0.00075
    output_tokens_pricing: 0.00100

report:
  summary_text: 'The average inference latency for this workload with prompt tokens {avg_prompt_token_count} (p95 is {p95_prompt_token_count}) and completion tokens {avg_completion_token_count} (p95 is {p95_completion_token_count}) when using {model_id} is {avg_latency}s (p95 is {p95_latency}s) and the average cost for 10,000 requests is ${avg_cost} (p95 is ${p95_cost_per_txn}), this is based on {count} requests.' 
